{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Data Exploration\n",
    "## Data provenance and characteristics:\n",
    " \n",
    "### When and from where it was collected:\n",
    "Data was collected from the GitHub presented in moodle:  https://github.com/Jl-wei/APIA2022-French-user-reviews-classification-dataset\n",
    "\n",
    "### Text genre(s) and language(s) it covers\n",
    "The data represents french user reviews from three applications on Google Play: Garmin Connect, Huawei Health and Samsung Health\n",
    "\n",
    "### How it has been annotated\n",
    "Each entry in the data set is annotated with four labels: \"Rating\", \"User Experience\", \"Feature request\" and \"Bug Report\"\n",
    "It also includes the score given by the user and the written review.\n",
    "We can see here the data organized per app and type of review.\n",
    "\n",
    "| App            | Total | Rating | Bug report | Feature request | User experience |\n",
    "| -------------- | ----- | ------ | ---------- | --------------- | --------------- |\n",
    "| Garmin Connect | 2000  | 1260   | 757        | 170             | 493             |\n",
    "| Huawei Health  | 2000  | 1068   | 819        | 384             | 289             |\n",
    "| Samsung Health | 2000  | 1324   | 491        | 486             | 349             |\n",
    "\n",
    "\n",
    "\n",
    "### Importing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the data from the file\n",
    "data_garmin_df = pd.read_csv('C:/Users/afons/Documents/2022-2023/2nd_semester/PLN/NLP/data/Garmin_Connect.csv')\n",
    "data_samsung_df = pd.read_csv('C:/Users/afons/Documents/2022-2023/2nd_semester/PLN/NLP/data/Samsung_Health.csv')\n",
    "data_huawei_df = pd.read_csv('C:/Users/afons/Documents/2022-2023/2nd_semester/PLN/NLP/data/Huawei_Health.csv')\n",
    "\n",
    "data = pd.concat([data_garmin_df, data_samsung_df, data_huawei_df], ignore_index=True)\n",
    "\n",
    "print(data.head())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Amount of examples per class\n",
    "We can conclude that we have a solid amount of data in each class, which will help us in the classification stage. Despite this there are some classes like rating with a lot more examples then feature request, we will have to keep that in mind and use stratification to guarantee a representative amount of data in the training and testing sets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# print amount of examples per class in the dataset\n",
    "print(\"Number of examples: \", len(data))\n",
    "print(\"Rating: \", len(data[data['rating'] == 1]))\n",
    "print(\"User Experience: \", len(data[data['user_experience'] == 1]))\n",
    "print(\"Bug Report: \", len(data[data['bug_report'] == 1]))\n",
    "print(\"Feature Request: \", len(data[data['feature_request'] == 1]))\n",
    "\n",
    "fig_size = plt.rcParams[\"figure.figsize\"]\n",
    "fig_size[0] = 10\n",
    "fig_size[1] = 8\n",
    "plt.rcParams[\"figure.figsize\"] = fig_size\n",
    "\n",
    "data_labels = data[[\"rating\", \"bug_report\", \"feature_request\", \"user_experience\"]]\n",
    "\n",
    "data_labels.sum(axis=0).plot.bar()\n",
    "\n",
    "# combine all text columns into one list\n",
    "raw_text = \" \".join(data['data'].tolist())\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the rating label is much more relevant than the other 3 labels, which causes unbalance and might mislead the results if not careful. Also, it seems that there are cases where more than 1 label can be applied. This makes it a multilabel problem:\n",
    "\n",
    "![Types of classification problems (https://towardsdatascience.com/multilabel-text-classification-done-right-using-scikit-learn-and-stacked-generalization-f5df2defc3b5#6de1)](./data/Types_of_classification_problems.png \"Types of classification problems\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also explored the overlap of classes, since this is a multilabel classification problem. We concluded that still most of the reviews are only ratings, the biggest overlapping was rating and bug report. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "tags = []\n",
    "\n",
    "for index, row in data.iterrows():\n",
    "    str_tag = \"\"\n",
    "    if row[\"rating\"] == 1:\n",
    "        str_tag += \"rating,\"\n",
    "    if row[\"bug_report\"] == 1:\n",
    "        str_tag += \"bug_report,\"\n",
    "    if row[\"feature_request\"] == 1:\n",
    "        str_tag += \"feature_request,\"\n",
    "    if row[\"user_experience\"] == 1:\n",
    "        str_tag += \"user_experience,\"\n",
    "\n",
    "    list_tag = str_tag[0:-1].split(',')\n",
    "\n",
    "    tags.append(list_tag)\n",
    "\n",
    "data = data.assign(tags=tags)\n",
    "\n",
    "\n",
    "mlb = MultiLabelBinarizer()\n",
    "\n",
    "tags1 = mlb.fit_transform(data[\"tags\"])\n",
    "\n",
    "data[\"tags\"].value_counts().sort_index().plot.bar(x=\"Tag Distribution of All Observations\", y=\"Number of observations\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Word distribution (TF-IDF)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We calculated the most frequent tokens in the reviews and developed a word cloud to help visualize it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# tokenize the text\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def flatten(l):\n",
    "    return [item for sublist in l for item in sublist]\n",
    "\n",
    "tokenized_text = flatten(data['data'].apply(word_tokenize).tolist())\n",
    "\n",
    "print(tokenized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "unique_tokens = set(tokenized_text)\n",
    "\n",
    "count_dict = {}\n",
    "for type in unique_tokens:\n",
    "    count_dict[type] = raw_text.count(type)\n",
    "\n",
    "token_counter = Counter(count_dict)\n",
    "print(token_counter.most_common(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "wordcloud = WordCloud().generate(\" \".join(tokenized_text))\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We created a box plot to visualize if there is any relationship between score and the type of review and concluded we can extract information from the score because different type of reviews were given different scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_columns = ['rating', 'bug_report', 'feature_request', 'user_experience']\n",
    "\n",
    "# Create a box plot to show the distribution of scores for each type of comment\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "ax.boxplot([data[data[col]==1]['score'] for col in score_columns], labels=score_columns)\n",
    "ax.set_xlabel('Type of Comment')\n",
    "ax.set_ylabel('Score')\n",
    "ax.set_title('Score by Type of Comment')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9650cb4e16cdd4a8e8e2d128bf38d875813998db22a3c986335f89e0cb4d7bb2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
