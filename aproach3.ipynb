{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithm Adaptation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "pd.set_option('display.max_colwidth', 100)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre processing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                                  data  \\\n",
      "0  Contrairement aux idées reçues le traceur GPS est très précis, dumoins avec vivoactive 3 Music.....   \n",
      "1                                                                 Application très pratique et fiable.   \n",
      "2                                                                                     jadore ma montre   \n",
      "3                                Super application, je l'utilise synchronisé avec ma fenix3 et j'adore   \n",
      "4                                                                                              Super !   \n",
      "\n",
      "   score  rating  bug_report  feature_request  user_experience  \n",
      "0      5       1           0                0                1  \n",
      "1      5       1           0                0                0  \n",
      "2      5       1           0                0                0  \n",
      "3      5       1           0                0                1  \n",
      "4      5       1           0                0                0  \n"
     ]
    }
   ],
   "source": [
    "data_garmin_df = pd.read_csv('data/Garmin_Connect.csv')\n",
    "data_samsung_df = pd.read_csv('data/Samsung_Health.csv')\n",
    "data_huawei_df = pd.read_csv('data/Huawei_Health.csv')\n",
    "\n",
    "data = pd.concat([data_garmin_df, data_samsung_df, data_huawei_df], ignore_index=True)\n",
    "\n",
    "print(data.head())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Employing non-alphabetic filtering, lowercasing, stop word removal, and stemming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    contrair id re ue traceur gp tr pr ci dumoin vivoact music tr motiv suit conseil garmin\n",
      "1                                                                    applic tr pratiqu fiabl\n",
      "2                                                                                jador montr\n",
      "3                                                   super applic utilis synchroni fenix ador\n",
      "4                                                                                      super\n",
      "Name: text_clean, dtype: object\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "corpus = []\n",
    "ps = PorterStemmer()\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "\n",
    "def clean_text(text):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    sw = set(stopwords.words('french'))\n",
    "    # get review and remove non alpha chars\n",
    "    review = re.sub('[^a-zA-Z]', ' ', text)\n",
    "    # to lower-case\n",
    "    review = review.lower()\n",
    "    # split into tokens, apply stemming and remove stop words\n",
    "    #review = ' '.join([lemmatizer.lemmatize(w) for w in review.split()])\n",
    "    return ' '.join([ps.stem(w) for w in review.split() if w not in sw])\n",
    "\n",
    "\n",
    "data['text_clean'] = data['data'].apply(lambda x: clean_text(x))\n",
    "print(data['text_clean'].head())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting into test and train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3682    aim bien cett applic comm a peut dire combien a fait mar v lo sport plein autr aussi pouvez choisir\n",
      "3067                                                                                 bonn applic fi tr bien\n",
      "2487                                                          dommag puiss mesur stress rythm cardiaqu note\n",
      "4622                                                                                                  super\n",
      "5160                                                  fonctionn bien depui mise jour r veill matin pad fait\n",
      "                                                       ...                                                 \n",
      "420                                                                                                    cool\n",
      "5848                                                                              temp sommeil rest parfait\n",
      "5191                                          a nouveau acc diff rent fond cran plu connexion phone one plu\n",
      "3166                                                                                          tr bonn appli\n",
      "3879                                                                                                    top\n",
      "Name: text_clean, Length: 4800, dtype: object       rating  bug_report  feature_request  user_experience\n",
      "3682       1           0                0                1\n",
      "3067       1           0                0                1\n",
      "2487       0           0                1                0\n",
      "4622       1           0                0                0\n",
      "5160       0           1                0                0\n",
      "...      ...         ...              ...              ...\n",
      "420        1           0                0                0\n",
      "5848       1           0                1                0\n",
      "5191       0           1                0                0\n",
      "3166       1           0                0                0\n",
      "3879       1           0                0                0\n",
      "\n",
      "[4800 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "y = data[['rating','bug_report','feature_request','user_experience']]\n",
    "\n",
    "# Split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(data['text_clean'],y , test_size=0.2,stratify=y, random_state=42)\n",
    "print(X_train,y_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Marta\n",
      "[nltk_data]     Mariz\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.pipeline import Pipeline\n",
    "nltk.download('stopwords')\n",
    "\n",
    "final_stopwords_list = stopwords.words('english') + stopwords.words('french')\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.8,\n",
    "                                   max_features=200000,\n",
    "                                   min_df=0.2,\n",
    "                                   stop_words=final_stopwords_list,\n",
    "                                   use_idf=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xfeatures = tfidf_vectorizer.fit_transform(X_train).toarray()\n",
    "XTest = tfidf_vectorizer.fit_transform(X_test).toarray()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'precision': 0.2816000845487212, 'recall': 0.4629042485732403, 'f1': 0.35017627094141496}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Marta Mariz\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "scores = pd.DataFrame()\n",
    "\n",
    "def score(y_true, y_pred):\n",
    "    \"\"\"Calculate precision, recall, and f1 score\"\"\"\n",
    "\n",
    "    metrics = precision_recall_fscore_support(y_true, y_pred, average='weighted')\n",
    "    performance = {'precision': metrics[0], 'recall': metrics[1], 'f1': metrics[2]}\n",
    "    return performance\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "clf = DecisionTreeClassifier(random_state=0)\n",
    "\n",
    "clf.fit(Xfeatures, y_train)\n",
    "y_pred = clf.predict(XTest)\n",
    "\n",
    "print(score(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "y should be a 1d array, got an array of shape (4800, 4) instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtree\u001b[39;00m \u001b[39mimport\u001b[39;00m ExtraTreeClassifier\n\u001b[0;32m      4\u001b[0m extra_tree \u001b[39m=\u001b[39m ExtraTreeClassifier(random_state\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[1;32m----> 5\u001b[0m \u001b[39mcls\u001b[39m \u001b[39m=\u001b[39m BaggingClassifier(extra_tree, random_state\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m)\u001b[39m.\u001b[39;49mfit(Xfeatures, y_train)\n\u001b[0;32m      6\u001b[0m y_pred \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39mpredict(XTest)\n\u001b[0;32m      7\u001b[0m \u001b[39mprint\u001b[39m(score(y_test, y_pred))\n",
      "File \u001b[1;32mc:\\Users\\Marta Mariz\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\ensemble\\_bagging.py:337\u001b[0m, in \u001b[0;36mBaseBagging.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    328\u001b[0m \u001b[39m# Convert data (X is required to be 2d and indexable)\u001b[39;00m\n\u001b[0;32m    329\u001b[0m X, y \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_data(\n\u001b[0;32m    330\u001b[0m     X,\n\u001b[0;32m    331\u001b[0m     y,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    335\u001b[0m     multi_output\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[0;32m    336\u001b[0m )\n\u001b[1;32m--> 337\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit(X, y, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_samples, sample_weight\u001b[39m=\u001b[39;49msample_weight)\n",
      "File \u001b[1;32mc:\\Users\\Marta Mariz\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\ensemble\\_bagging.py:393\u001b[0m, in \u001b[0;36mBaseBagging._fit\u001b[1;34m(self, X, y, max_samples, max_depth, sample_weight, check_input)\u001b[0m\n\u001b[0;32m    391\u001b[0m n_samples \u001b[39m=\u001b[39m X\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]\n\u001b[0;32m    392\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_n_samples \u001b[39m=\u001b[39m n_samples\n\u001b[1;32m--> 393\u001b[0m y \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_y(y)\n\u001b[0;32m    395\u001b[0m \u001b[39m# Check parameters\u001b[39;00m\n\u001b[0;32m    396\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_estimator()\n",
      "File \u001b[1;32mc:\\Users\\Marta Mariz\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\ensemble\\_bagging.py:802\u001b[0m, in \u001b[0;36mBaggingClassifier._validate_y\u001b[1;34m(self, y)\u001b[0m\n\u001b[0;32m    801\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_validate_y\u001b[39m(\u001b[39mself\u001b[39m, y):\n\u001b[1;32m--> 802\u001b[0m     y \u001b[39m=\u001b[39m column_or_1d(y, warn\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[0;32m    803\u001b[0m     check_classification_targets(y)\n\u001b[0;32m    804\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclasses_, y \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39munique(y, return_inverse\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\Marta Mariz\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py:1202\u001b[0m, in \u001b[0;36mcolumn_or_1d\u001b[1;34m(y, dtype, warn)\u001b[0m\n\u001b[0;32m   1193\u001b[0m         warnings\u001b[39m.\u001b[39mwarn(\n\u001b[0;32m   1194\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mA column-vector y was passed when a 1d array was\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1195\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39m expected. Please change the shape of y to \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1198\u001b[0m             stacklevel\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m,\n\u001b[0;32m   1199\u001b[0m         )\n\u001b[0;32m   1200\u001b[0m     \u001b[39mreturn\u001b[39;00m _asarray_with_order(xp\u001b[39m.\u001b[39mreshape(y, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m), order\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mC\u001b[39m\u001b[39m\"\u001b[39m, xp\u001b[39m=\u001b[39mxp)\n\u001b[1;32m-> 1202\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m   1203\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39my should be a 1d array, got an array of shape \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m instead.\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(shape)\n\u001b[0;32m   1204\u001b[0m )\n",
      "\u001b[1;31mValueError\u001b[0m: y should be a 1d array, got an array of shape (4800, 4) instead."
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import ExtraTreeClassifier\n",
    "\n",
    "extra_tree = ExtraTreeClassifier(random_state=0)\n",
    "cls = BaggingClassifier(extra_tree, random_state=0).fit(Xfeatures, y_train)\n",
    "y_pred = cls.predict(XTest)\n",
    "print(score(y_test, y_pred))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bcfe8c9e21b55aa01906cc0de6370d442d81bec12f7f116beb357c60c30ba9f9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
