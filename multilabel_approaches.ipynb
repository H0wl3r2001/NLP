{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing the data \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                data  score  rating  \\\n",
      "0  Contrairement aux idées reçues le traceur GPS ...      5       1   \n",
      "1               Application très pratique et fiable.      5       1   \n",
      "2                                   jadore ma montre      5       1   \n",
      "3  Super application, je l'utilise synchronisé av...      5       1   \n",
      "4                                            Super !      5       1   \n",
      "5  Application très pratique et très simple d'uti...      3       1   \n",
      "6  Suivis du sommeil cardio nombre de pas avec la...      5       1   \n",
      "7                                  Sympa et précis !      5       1   \n",
      "8                                     Très satisfait      5       1   \n",
      "9  bonjour, le calendrier ne se synchronise plus....      2       0   \n",
      "\n",
      "   bug_report  feature_request  user_experience  \n",
      "0           0                0                1  \n",
      "1           0                0                0  \n",
      "2           0                0                0  \n",
      "3           0                0                1  \n",
      "4           0                0                0  \n",
      "5           1                0                0  \n",
      "6           0                0                1  \n",
      "7           0                0                0  \n",
      "8           0                0                0  \n",
      "9           1                0                0  \n",
      "\n",
      " Number of rows: 6000\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "import nltk\n",
    "import pandas as pd\n",
    "\n",
    "# Read the data from the file\n",
    "data_garmin_df = pd.read_csv('data/Garmin_Connect.csv')\n",
    "data_samsung_df = pd.read_csv('data/Samsung_Health.csv')\n",
    "data_huawei_df = pd.read_csv('data/Huawei_Health.csv')\n",
    "\n",
    "data = pd.concat([data_garmin_df, data_samsung_df, data_huawei_df], ignore_index=True)\n",
    "#data.to_csv('data/concatenated_data.csv', sep='\\t', encoding='utf-8')\n",
    "\n",
    "print(data.head(10))\n",
    "print(\"\\n Number of rows: \" + str(len(data)))\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization and removal of stopwords\n",
    "\n",
    "*Tokenization* is the process of splitting an input text into tokens (words or other relevant elements, such as punctuation, empty strings). We will use the result as a basis to predict a label.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Marta\n",
      "[nltk_data]     Mariz\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                data  score  rating  \\\n",
      "0  Contrairement aux idées reçues le traceur GPS ...      5       1   \n",
      "1               Application très pratique et fiable.      5       1   \n",
      "2                                   jadore ma montre      5       1   \n",
      "3  Super application, je l'utilise synchronisé av...      5       1   \n",
      "4                                            Super !      5       1   \n",
      "\n",
      "   bug_report  feature_request  user_experience  \\\n",
      "0           0                0                1   \n",
      "1           0                0                0   \n",
      "2           0                0                0   \n",
      "3           0                0                1   \n",
      "4           0                0                0   \n",
      "\n",
      "                                               token  \n",
      "0  contrair aux id e re ue le traceur gp est tr s...  \n",
      "1                      appliqu tr s pratiqu et fiabl  \n",
      "2                                     jador ma montr  \n",
      "3  sup appliqu je l utilis synchron avec ma fenix...  \n",
      "4                                                sup  \n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize #principal tokenization class from nltk API\n",
    "from nltk.stem import SnowballStemmer   #Stemming method\n",
    "import re                               #regex library\n",
    "nltk.download('punkt')\n",
    "\n",
    "corpus = []\n",
    "\n",
    "for index, row in data.iterrows():\n",
    "    review = re.sub('\\*', '', row[\"data\"]) # get data, substitute asterisks for empty string, put into review\n",
    "    review = re.sub('[^a-zA-Z]', ' ', review) # from review, remove all non-alphabetic characters\n",
    "    review = re.sub('[^\\w\\s]', '', review) # remove punctuation from review\n",
    "    review = ' '.join([SnowballStemmer('french').stem(w) for w in word_tokenize(review.lower(), language='french')]) # apply stemming\n",
    "    corpus.append(review)\n",
    "\n",
    "#print(corpus)\n",
    "\n",
    "data = data.assign(token=corpus)\n",
    "\n",
    "print(data.head())\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separation between train and test datasets\n",
    "\n",
    "Separate in adequate proportions to avoid the overfitting of the modules the data between features and targets. In this case there will be 2 different separations, one for the original multilabel problem and another for the mold into just a multiclass problem. To ensure a more even tag distribution, we must use the *stratify* hyper-parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skmultilearn.model_selection import iterative_train_test_split\n",
    "from sklearn.model_selection import train_test_split\n",
    "x = data[['token']]  # try to include / exclude score and check if it yields better results\n",
    "y = data[['rating', 'bug_report', 'feature_request', 'user_experience']]\n",
    "\n",
    "X_train, y_train, X_test, y_test = train_test_split(x, y, stratify=y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Approaches to multilabel classification we will consider:\n",
    "\n",
    "#### Problem transformation\n",
    "1. Binary Relevance (consider each label as a separate single class classification  problem)\n",
    "2. Classifier Chains\n",
    "3. Label powerset\n",
    "\n",
    "#### Adapted Algorithms\n",
    "\n",
    "#### Ensemble methods"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TF-IDF Vectorizer setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Marta\n",
      "[nltk_data]     Mariz\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "final_stopwords_list = stopwords.words('english') + stopwords.words('french')\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.5,\n",
    "                                   max_features=200000,\n",
    "                                   min_df=0.2,\n",
    "                                   stop_words=final_stopwords_list,\n",
    "                                   use_idf=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "# sklearn reference: https://scikit-learn.org/\n",
    "# pandas reference: https://pandas.pydata.org/\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn import metrics\n",
    "import pandas as pd\n",
    "\n",
    "# text and numeric classes that use sklearn base libaries\n",
    "class TextTransformer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Transform text features\n",
    "    \"\"\"\n",
    "    def __init__(self, key):\n",
    "        self.key = key\n",
    "\n",
    "    def fit(self, X, y=None, *parg, **kwarg):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return X[self.key]\n",
    "    \n",
    "class NumberTransformer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Transform numeric features\n",
    "    \"\"\"\n",
    "    def __init__(self, key):\n",
    "        self.key = key\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return X[[self.key]]\n",
    "\n",
    "\n",
    "# use the term-frequency inverse document frequency vectorizer to transfrom count of text\n",
    "# into a weighed matrix of term importance\n",
    "\n",
    "\n",
    "# compile both the TextTransformer and TfidfVectorizer \n",
    "# to the text 'Text_Feature' \n",
    "color_text = Pipeline([\n",
    "                ('transformer', TextTransformer(key='token')),\n",
    "                ('vectorizer', tfidf_vectorizer)\n",
    "                ])\n",
    "\n",
    "# compile the NumberTransformer to the 'score' numeric feature\n",
    "score_numeric = Pipeline([\n",
    "                ('transformer', NumberTransformer(key='score')),\n",
    "                ])\n",
    "\n",
    "\n",
    "\n",
    "# combine all of the features, text and numeric together\n",
    "features = FeatureUnion([('Token_Feature', color_text),\n",
    "                         ('Confirmed_Score', score_numeric),\n",
    "                        ])\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem transformation: Binary Relevance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "After pruning, no terms remain. Try a lower min_df or a higher max_df.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 15\u001b[0m\n\u001b[0;32m      9\u001b[0m pipe \u001b[39m=\u001b[39m Pipeline([(\u001b[39m'\u001b[39m\u001b[39mtfidf\u001b[39m\u001b[39m'\u001b[39m, tfidf_vectorizer),\n\u001b[0;32m     10\u001b[0m                  (\u001b[39m'\u001b[39m\u001b[39mclf\u001b[39m\u001b[39m'\u001b[39m,clf)\n\u001b[0;32m     11\u001b[0m                  ])\n\u001b[0;32m     14\u001b[0m \u001b[39m# fit the model\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m pipe\u001b[39m.\u001b[39;49mfit(X_train, y_train)\n\u001b[0;32m     17\u001b[0m \u001b[39m# predict from the test set\u001b[39;00m\n\u001b[0;32m     18\u001b[0m preds \u001b[39m=\u001b[39m pipe\u001b[39m.\u001b[39mpredict(X_test)\n",
      "File \u001b[1;32mc:\\Users\\Marta Mariz\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\pipeline.py:401\u001b[0m, in \u001b[0;36mPipeline.fit\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    375\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Fit the model.\u001b[39;00m\n\u001b[0;32m    376\u001b[0m \n\u001b[0;32m    377\u001b[0m \u001b[39mFit all the transformers one after the other and transform the\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    398\u001b[0m \u001b[39m    Pipeline with fitted steps.\u001b[39;00m\n\u001b[0;32m    399\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    400\u001b[0m fit_params_steps \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_fit_params(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params)\n\u001b[1;32m--> 401\u001b[0m Xt \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fit(X, y, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params_steps)\n\u001b[0;32m    402\u001b[0m \u001b[39mwith\u001b[39;00m _print_elapsed_time(\u001b[39m\"\u001b[39m\u001b[39mPipeline\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_log_message(\u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msteps) \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m)):\n\u001b[0;32m    403\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_final_estimator \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mpassthrough\u001b[39m\u001b[39m\"\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\Marta Mariz\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\pipeline.py:359\u001b[0m, in \u001b[0;36mPipeline._fit\u001b[1;34m(self, X, y, **fit_params_steps)\u001b[0m\n\u001b[0;32m    357\u001b[0m     cloned_transformer \u001b[39m=\u001b[39m clone(transformer)\n\u001b[0;32m    358\u001b[0m \u001b[39m# Fit or load from cache the current transformer\u001b[39;00m\n\u001b[1;32m--> 359\u001b[0m X, fitted_transformer \u001b[39m=\u001b[39m fit_transform_one_cached(\n\u001b[0;32m    360\u001b[0m     cloned_transformer,\n\u001b[0;32m    361\u001b[0m     X,\n\u001b[0;32m    362\u001b[0m     y,\n\u001b[0;32m    363\u001b[0m     \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m    364\u001b[0m     message_clsname\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mPipeline\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    365\u001b[0m     message\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_log_message(step_idx),\n\u001b[0;32m    366\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params_steps[name],\n\u001b[0;32m    367\u001b[0m )\n\u001b[0;32m    368\u001b[0m \u001b[39m# Replace the transformer of the step with the fitted\u001b[39;00m\n\u001b[0;32m    369\u001b[0m \u001b[39m# transformer. This is necessary when loading the transformer\u001b[39;00m\n\u001b[0;32m    370\u001b[0m \u001b[39m# from the cache.\u001b[39;00m\n\u001b[0;32m    371\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msteps[step_idx] \u001b[39m=\u001b[39m (name, fitted_transformer)\n",
      "File \u001b[1;32mc:\\Users\\Marta Mariz\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\joblib\\memory.py:349\u001b[0m, in \u001b[0;36mNotMemorizedFunc.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    348\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m--> 349\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfunc(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Marta Mariz\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\pipeline.py:893\u001b[0m, in \u001b[0;36m_fit_transform_one\u001b[1;34m(transformer, X, y, weight, message_clsname, message, **fit_params)\u001b[0m\n\u001b[0;32m    891\u001b[0m \u001b[39mwith\u001b[39;00m _print_elapsed_time(message_clsname, message):\n\u001b[0;32m    892\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(transformer, \u001b[39m\"\u001b[39m\u001b[39mfit_transform\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m--> 893\u001b[0m         res \u001b[39m=\u001b[39m transformer\u001b[39m.\u001b[39mfit_transform(X, y, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params)\n\u001b[0;32m    894\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    895\u001b[0m         res \u001b[39m=\u001b[39m transformer\u001b[39m.\u001b[39mfit(X, y, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params)\u001b[39m.\u001b[39mtransform(X)\n",
      "File \u001b[1;32mc:\\Users\\Marta Mariz\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:2131\u001b[0m, in \u001b[0;36mTfidfVectorizer.fit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   2124\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_params()\n\u001b[0;32m   2125\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tfidf \u001b[39m=\u001b[39m TfidfTransformer(\n\u001b[0;32m   2126\u001b[0m     norm\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnorm,\n\u001b[0;32m   2127\u001b[0m     use_idf\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39muse_idf,\n\u001b[0;32m   2128\u001b[0m     smooth_idf\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msmooth_idf,\n\u001b[0;32m   2129\u001b[0m     sublinear_tf\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msublinear_tf,\n\u001b[0;32m   2130\u001b[0m )\n\u001b[1;32m-> 2131\u001b[0m X \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mfit_transform(raw_documents)\n\u001b[0;32m   2132\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tfidf\u001b[39m.\u001b[39mfit(X)\n\u001b[0;32m   2133\u001b[0m \u001b[39m# X is already a transformed view of raw_documents so\u001b[39;00m\n\u001b[0;32m   2134\u001b[0m \u001b[39m# we set copy to False\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Marta Mariz\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1400\u001b[0m, in \u001b[0;36mCountVectorizer.fit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1398\u001b[0m \u001b[39mif\u001b[39;00m max_features \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   1399\u001b[0m     X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sort_features(X, vocabulary)\n\u001b[1;32m-> 1400\u001b[0m X, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstop_words_ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_limit_features(\n\u001b[0;32m   1401\u001b[0m     X, vocabulary, max_doc_count, min_doc_count, max_features\n\u001b[0;32m   1402\u001b[0m )\n\u001b[0;32m   1403\u001b[0m \u001b[39mif\u001b[39;00m max_features \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   1404\u001b[0m     X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sort_features(X, vocabulary)\n",
      "File \u001b[1;32mc:\\Users\\Marta Mariz\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1252\u001b[0m, in \u001b[0;36mCountVectorizer._limit_features\u001b[1;34m(self, X, vocabulary, high, low, limit)\u001b[0m\n\u001b[0;32m   1250\u001b[0m kept_indices \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mwhere(mask)[\u001b[39m0\u001b[39m]\n\u001b[0;32m   1251\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(kept_indices) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m-> 1252\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m   1253\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAfter pruning, no terms remain. Try a lower min_df or a higher max_df.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1254\u001b[0m     )\n\u001b[0;32m   1255\u001b[0m \u001b[39mreturn\u001b[39;00m X[:, kept_indices], removed_terms\n",
      "\u001b[1;31mValueError\u001b[0m: After pruning, no terms remain. Try a lower min_df or a higher max_df."
     ]
    }
   ],
   "source": [
    "# using binary relevance\n",
    "from skmultilearn.problem_transform import BinaryRelevance\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# create the classfier from RF\n",
    "clf = BinaryRelevance(GaussianNB())\n",
    "\n",
    "# unite the features and classfier together\n",
    "pipe = Pipeline([('tfidf', tfidf_vectorizer),\n",
    "                 ('clf',clf)\n",
    "                 ])\n",
    "\n",
    "\n",
    "# fit the model\n",
    "pipe.fit(X_train, y_train)\n",
    "\n",
    "# predict from the test set\n",
    "preds = pipe.predict(X_test)\n",
    "\n",
    "# print out your accuracy\n",
    "print(\"Accuracy:\",metrics.accuracy_score(y_test, preds))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bcfe8c9e21b55aa01906cc0de6370d442d81bec12f7f116beb357c60c30ba9f9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
