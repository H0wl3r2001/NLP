{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing the data \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                data  score  rating  \\\n",
      "0  Contrairement aux idées reçues le traceur GPS ...      5       1   \n",
      "1               Application très pratique et fiable.      5       1   \n",
      "2                                   jadore ma montre      5       1   \n",
      "3  Super application, je l'utilise synchronisé av...      5       1   \n",
      "4                                            Super !      5       1   \n",
      "5  Application très pratique et très simple d'uti...      3       1   \n",
      "6  Suivis du sommeil cardio nombre de pas avec la...      5       1   \n",
      "7                                  Sympa et précis !      5       1   \n",
      "8                                     Très satisfait      5       1   \n",
      "9  bonjour, le calendrier ne se synchronise plus....      2       0   \n",
      "\n",
      "   bug_report  feature_request  user_experience  \n",
      "0           0                0                1  \n",
      "1           0                0                0  \n",
      "2           0                0                0  \n",
      "3           0                0                1  \n",
      "4           0                0                0  \n",
      "5           1                0                0  \n",
      "6           0                0                1  \n",
      "7           0                0                0  \n",
      "8           0                0                0  \n",
      "9           1                0                0  \n",
      "\n",
      " Number of rows: 6000\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import gensim\n",
    "import pandas as pd\n",
    "\n",
    "# Read the data from the file\n",
    "data_garmin_df = pd.read_csv('data/Garmin_Connect.csv')\n",
    "data_samsung_df = pd.read_csv('data/Samsung_Health.csv')\n",
    "data_huawei_df = pd.read_csv('data/Huawei_Health.csv')\n",
    "\n",
    "data = pd.concat([data_garmin_df, data_samsung_df, data_huawei_df], ignore_index=True)\n",
    "#data.to_csv('data/concatenated_data.csv', sep='\\t', encoding='utf-8')\n",
    "\n",
    "print(data.head(10))\n",
    "print(\"\\n Number of rows: \" + str(len(data)))\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization and removal of stopwords\n",
    "\n",
    "*Tokenization* is the process of splitting an input text into tokens (words or other relevant elements, such as punctuation, empty strings). We will use the result as a basis to predict a label.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\radio\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                data  score  rating  \\\n",
      "0  Contrairement aux idées reçues le traceur GPS ...      5       1   \n",
      "1               Application très pratique et fiable.      5       1   \n",
      "2                                   jadore ma montre      5       1   \n",
      "3  Super application, je l'utilise synchronisé av...      5       1   \n",
      "4                                            Super !      5       1   \n",
      "\n",
      "   bug_report  feature_request  user_experience  \\\n",
      "0           0                0                1   \n",
      "1           0                0                0   \n",
      "2           0                0                0   \n",
      "3           0                0                1   \n",
      "4           0                0                0   \n",
      "\n",
      "                                               token  \n",
      "0  contrair aux id e re ue le traceur gp est tr s...  \n",
      "1                      appliqu tr s pratiqu et fiabl  \n",
      "2                                     jador ma montr  \n",
      "3  sup appliqu je l utilis synchron avec ma fenix...  \n",
      "4                                                sup  \n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize #principal tokenization class from nltk API\n",
    "from nltk.stem import SnowballStemmer   #Stemming method\n",
    "import re                               #regex library\n",
    "nltk.download('punkt')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "final_stopwords_list = stopwords.words('french') + stopwords.words('english')\n",
    "## IMPLEMENT LEMMATIZATION\n",
    "\n",
    "corpus = []\n",
    "\n",
    "for index, row in data.iterrows():\n",
    "    review = re.sub('\\*', '', row[\"data\"]) # get data, substitute asterisks for empty string, put into review\n",
    "    review = re.sub('[^a-zA-Z]', ' ', review) # from review, remove all non-alphabetic characters\n",
    "    review = re.sub('[^\\w\\s]', '', review) # remove punctuation from review\n",
    "    review = ' '.join([SnowballStemmer('french').stem(w) for w in word_tokenize(review.lower(), language='french')]) # apply stemming\n",
    "    corpus.append(review)\n",
    "\n",
    "#print(corpus)\n",
    "\n",
    "data = data.assign(token=corpus)\n",
    "\n",
    "print(data.head())\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separation between train and test datasets\n",
    "\n",
    "Separate in adequate proportions to avoid the overfitting of the modules the data between features and targets. In this case there will be 2 different separations, one for the original multilabel problem and another for the mold into just a multiclass problem. To ensure a more even tag distribution, we must use the *stratify* hyper-parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    [contrairement, idées, reçues, traceur, gps, t...\n",
      "1                [application, très, pratique, fiable]\n",
      "2                                     [jadore, montre]\n",
      "3    [super, application, utilise, synchronisé, fen...\n",
      "4                                              [super]\n",
      "Name: text_clean, dtype: object\n"
     ]
    }
   ],
   "source": [
    "from skmultilearn.model_selection import iterative_train_test_split\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_stemmed_lemmatized = data['token']  # try to include / exclude score and check if it yields better results\n",
    "data['text_clean'] = data['data'].apply(lambda x: gensim.utils.simple_preprocess(x))\n",
    "# remove stopwords\n",
    "data['text_clean'] = data['text_clean'].apply(lambda x: [item for item in x if item not in final_stopwords_list])\n",
    "x_basic_preprocessing = data['text_clean']\n",
    "\n",
    "y = data[['rating', 'bug_report', 'feature_request', 'user_experience']]\n",
    "\n",
    "X_train_tfidf, X_test_tfidf, y_train_tfidf, y_test_tfidf = train_test_split(x_stemmed_lemmatized, y, stratify=y, test_size=0.2, random_state=42)\n",
    "\n",
    "X_train_basic, X_test_basic, y_train_basic, y_test_basic = train_test_split(x_basic_preprocessing, y, stratify=y, test_size=0.2, random_state=42)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Approaches to multilabel classification we will consider:\n",
    "\n",
    "#### Problem transformation\n",
    "1. Binary Relevance (consider each label as a separate single class classification  problem)\n",
    "2. Classifier Chains\n",
    "3. Label powerset\n",
    "\n",
    "#### Adapted Algorithms\n",
    "\n",
    "#### Ensemble methods"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sparse Representation: Finding the best parameters for TF-IDF Transformer and CountVectorizer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These two preprocessing steps will be present in all of our sparse pipelines, so lets check what hyperparameters are best for them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Define the hyperparameters to tune for the TfidfTransformer, which will be the same for every method approach as this preprocessor is always used\n",
    "tfidf_params = {\n",
    "    'use_idf': [True, False],\n",
    "    'smooth_idf': [True, False],\n",
    "    'sublinear_tf': [True, False],\n",
    "    'norm': ['l1', 'l2']\n",
    "}\n",
    "\n",
    "#Define the hyperparameters to tune for the CountVectorizer, which will be the same for every method approach as this preprocessor is always used\n",
    "count_vectorizer_params = {\n",
    "    'max_df': [0.5, 0.75, 1.0],\n",
    "    'min_df': [0.2, 0.25, 0.3],\n",
    "    'max_features': [None, 10000, 20000],\n",
    "    'ngram_range': [(1, 1), (1, 2), (1, 3)],\n",
    "    'stop_words': [None, final_stopwords_list],\n",
    "}\n",
    "\n",
    "grid_params = {\n",
    "        'vect__max_df': count_vectorizer_params['max_df'],\n",
    "        'vect__ngram_range': count_vectorizer_params['ngram_range'],\n",
    "        'vect__min_df': count_vectorizer_params['min_df'],\n",
    "        'vect__stop_words': count_vectorizer_params['stop_words'],\n",
    "        'tfidf__use_idf': tfidf_params['use_idf'],\n",
    "        'tfidf__smooth_idf': tfidf_params['smooth_idf'],\n",
    "        'tfidf__sublinear_tf': tfidf_params['sublinear_tf'],\n",
    "        'tfidf__norm': tfidf_params['norm'],\n",
    "        \n",
    "}\n",
    "\n",
    "\n",
    "# Create the pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', RandomForestClassifier()),\n",
    "])\n",
    "\n",
    "# Create the grid search\n",
    "grid_search = GridSearchCV(pipeline, grid_params, cv=2, n_jobs=-1, verbose=1)\n",
    "\n",
    "# Fit the grid search\n",
    "grid_search.fit(X_train_tfidf, y_train_tfidf)\n",
    "\n",
    "# Print the best parameters\n",
    "print(\"Best parameters:\" , grid_search.best_params_)\n",
    "print(\"Best score: \", grid_search.best_score_)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best parameters for count_vectorizer and tf_idf_transformer\n",
    "So we don't have to run the gridSearch multiple times \n",
    "\n",
    "Best parameters: \n",
    "- 'tfidf__norm': 'l1'\n",
    "- 'tfidf__smooth_idf': True\n",
    "- 'tfidf__sublinear_tf': True\n",
    "- 'tfidf__use_idf': True\n",
    "- 'vect__max_df': 0.8\n",
    "- 'vect__min_df': 0.2\n",
    "- 'vect__ngram_range': (1, 2)\n",
    "- 'vect__stop_words': final_stopwords_list\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new count vectorizer with the best parameters\n",
    "count_vectorizer = CountVectorizer(\n",
    "    max_df=0.8,\n",
    "    ngram_range=(1, 3),\n",
    "    min_df=0.2,\n",
    "    stop_words=None,\n",
    ")\n",
    "\n",
    "tf_idf_transformer = TfidfTransformer(\n",
    "    use_idf=True,\n",
    "    smooth_idf=True,\n",
    "    sublinear_tf=True,\n",
    "    norm='l1',\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dense representation: Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Word2Vec model trained on the corpus\n",
    "\n",
    "w2v_model = gensim.models.Word2Vec(X_train_basic,\n",
    "                                   vector_size=100,\n",
    "                                   window=5,\n",
    "                                   min_count=2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use our trained Word2Vec model to find similarities between words. Let's find the most similar words for 'compatibilité' to make sure it is working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('course', 0.9959745407104492),\n",
       " ('entrainement', 0.9959266781806946),\n",
       " ('fait', 0.9959033727645874),\n",
       " ('fais', 0.9958943724632263),\n",
       " ('petit', 0.9958926439285278),\n",
       " ('parcours', 0.9958776235580444),\n",
       " ('si', 0.995867133140564),\n",
       " ('exercice', 0.995858907699585),\n",
       " ('activité', 0.9958510398864746),\n",
       " ('pense', 0.9958379864692688)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find the most similar words to \"compatibilité\" based on word vectors from our trained model\n",
    "w2v_model.wv.most_similar('compatibilité')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate aggregated sentence vectors based on the word vectors for each word in the sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\radio\\AppData\\Local\\Temp\\ipykernel_1028\\3368788182.py:3: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  X_train_vect = np.array([np.array([w2v_model.wv[i] for i in ls if i in words])\n",
      "C:\\Users\\radio\\AppData\\Local\\Temp\\ipykernel_1028\\3368788182.py:5: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  X_test_vect = np.array([np.array([w2v_model.wv[i] for i in ls if i in words])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "words = set(w2v_model.wv.index_to_key )\n",
    "X_train_vect = np.array([np.array([w2v_model.wv[i] for i in ls if i in words])\n",
    "                         for ls in X_train_basic])\n",
    "X_test_vect = np.array([np.array([w2v_model.wv[i] for i in ls if i in words])\n",
    "                         for ls in X_test_basic])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem: inconsistent number of features in each sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Why is the length of the sentence different than the length of the sentence vector?\n",
    "for i, v in enumerate(X_train_vect):\n",
    "    print(len(X_train_basic.iloc[i]), len(v))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we were to pass these values into a machine learning algorithm, it would throw an error, because of the inconsistent amount of features in each data entry. In order to combat this, we are going to take an element wise average: We saw that there are two word vectors for the first sentence. Each of those word vectors is of size 100 because that’s the way we set it when we trained our model. What we’re going to do is we’re going to average the first element across those four word vectors and store that as the first entry in our final vector. Then we’ll do the same thing for the second element, and for the third, and so on. What we’ll end up with is now a single vector of length 100 that represents each text by averaging those word vectors for the words that were represented in that text message."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute sentence vectors by averaging the word vectors for the words contained in the sentence\n",
    "X_train_vect_avg = []\n",
    "for v in X_train_vect:\n",
    "    if v.size:\n",
    "        X_train_vect_avg.append(v.mean(axis=0))\n",
    "    else:\n",
    "        X_train_vect_avg.append(np.zeros(100, dtype=float))\n",
    "        \n",
    "X_test_vect_avg = []\n",
    "for v in X_test_vect:\n",
    "    if v.size:\n",
    "        X_test_vect_avg.append(v.mean(axis=0))\n",
    "    else:\n",
    "        X_test_vect_avg.append(np.zeros(100, dtype=float))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking that our sentence word vectors are of consistent 100 length:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, v in enumerate(X_train_vect_avg):\n",
    "    print(len(X_train_basic.iloc[i]), len(v))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our approach to the machine learning pipeline is as follows: create a function that creates pipelines taking in the estimators we pass to it(this is how we'll implement the many approaches to multilabel classification) and choosing whether we want to use TF-IDF or Word2Vec model for the pre-processing step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn import metrics\n",
    "\n",
    "\n",
    "def make_pipeline(clf_param_grid, clf,pre_processing='tf_idf'):\n",
    "    pipeline = None\n",
    "    X_train = None\n",
    "    y_train = None\n",
    "    X_test = None\n",
    "    y_test = None\n",
    "    if pre_processing == 'tf_idf':\n",
    "        X_train = X_train_tfidf\n",
    "        y_train = y_train_tfidf\n",
    "        X_test = X_test_tfidf\n",
    "        y_test = y_test_tfidf\n",
    "        pipeline = Pipeline([\n",
    "            ('vect', count_vectorizer),\n",
    "            ('tfidf', tf_idf_transformer),\n",
    "            ('clf', clf)\n",
    "        ])\n",
    "    elif pre_processing == 'word2vec':\n",
    "        X_train = X_train_vect_avg\n",
    "        y_train = y_train_basic\n",
    "        X_test = X_test_vect_avg\n",
    "        y_test = y_test_basic\n",
    "       \n",
    "        pipeline = Pipeline([\n",
    "            ('clf', clf)\n",
    "        ])\n",
    "\n",
    "    # Define the GridSearchCV object to tune the hyperparameters\n",
    "    grid_params = [\n",
    "        {\n",
    "        **clf_param\n",
    "        }\n",
    "        for clf_param in clf_param_grid\n",
    "    ]\n",
    "\n",
    "    # Define the GridSearchCV object\n",
    "    grid_search = GridSearchCV(pipeline, param_grid=grid_params, cv=2, n_jobs=-1)\n",
    "\n",
    "\n",
    "    # Fit the GridSearchCV object to the training data\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "    # Get the best hyperparameters and best model\n",
    "    best_params = grid_search.best_params_\n",
    "    best_model = grid_search.best_estimator_\n",
    "    print(\"Best parameters: \", best_params)\n",
    "    \n",
    "\n",
    "    # Evaluate the best model on the test data\n",
    "    y_pred = best_model.predict(X_test)\n",
    "    print(\"Accuracy: \", metrics.accuracy_score(y_test, y_pred))\n",
    "    print(y_pred.count_nonzero())\n",
    "    #Apply classification report\n",
    "    print(metrics.classification_report(y_test, y_pred, digits=3, target_names=['rating', 'bug_report', 'feature_request', 'user_experience']))\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adaptive algorithms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using binary relevance\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.tree import ExtraTreeClassifier\n",
    "from sklearn.linear_model import RidgeClassifierCV\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "clf = DecisionTreeClassifier() # what is this?\n",
    "\n",
    "param_grid_adaptive_algorithms = [\n",
    "    {\n",
    "        'clf': [DecisionTreeClassifier()],\n",
    "        'clf__criterion': [\"gini\", \"entropy\", \"log_loss\"],\n",
    "        'clf__min_samples_split': [1, 2, 10, 20],\n",
    "        'clf__max_depth': [None, 1, 2, 5, 10, 20, 50, 100],\n",
    "        'clf__class_weight': [None, 'balanced'],\n",
    "        'clf__random_state': [None, 42],\n",
    "        'clf__max_features': [None, 'auto', 'sqrt', 'log2'],\n",
    "    },\n",
    "    {\n",
    "        'clf': [ExtraTreeClassifier()],\n",
    "        'clf__criterion': [\"gini\", \"entropy\", \"log_loss\"],\n",
    "        'clf__min_samples_split': [1, 2, 10, 20],\n",
    "        'clf__max_depth': [None, 1, 2, 5, 10, 20, 50, 100],\n",
    "        'clf__class_weight': [None, 'balanced'],\n",
    "        'clf__random_state': [None, 42],\n",
    "        'clf__max_features': [None, 'auto', 'sqrt', 'log2'],\n",
    "    },\n",
    "    {\n",
    "        'clf': [KNeighborsClassifier()],\n",
    "        'clf__n_neighbors': [1, 2, 5, 10, 20, 50, 100],\n",
    "        'clf__weights': ['uniform', 'distance'],\n",
    "        'clf__algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute'],\n",
    "        'clf__metric': ['minkowski', 'euclidean', 'manhattan'],\n",
    "        'clf__p': [1, 2, 3, 4, 5],        \n",
    "    },\n",
    "    {\n",
    "        'clf': [RidgeClassifierCV()],\n",
    "        'clf__alphas': [0.1, 1.0, 10.0],\n",
    "        'clf__fit_intercept': [True, False],\n",
    "        'clf__normalize': [True, False],\n",
    "        'clf__scoring': [None, 'accuracy', 'precision', 'recall', 'f1'],\n",
    "        'clf__cv': [None, 3, 5, 10],\n",
    "        'clf__class_weight': [None, 'balanced'],\n",
    "        'clf__store_cv_values': [False, True],\n",
    "    }  \n",
    "]\n",
    "\n",
    "make_pipeline(param_grid_adaptive_algorithms, clf)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem transformation: Binary Relevance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using binary relevance\n",
    "from skmultilearn.problem_transform import BinaryRelevance\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "clf = BinaryRelevance(GaussianNB())\n",
    "\n",
    "param_grid_binary_relevance = [\n",
    "    {\n",
    "        'clf': [clf],\n",
    "        'clf__classifier__var_smoothing': [1e-9, 1e-8]\n",
    "    },\n",
    "    {\n",
    "        'clf': [BinaryRelevance(RandomForestClassifier(), require_dense=[False, True])],\n",
    "        'clf__classifier__n_estimators': [200, 250, 300],\n",
    "        'clf__classifier__max_depth': [10, 20],\n",
    "    },\n",
    "    #{\n",
    "    #    'clf': [BinaryRelevance(LinearSVC(), require_dense=[False, True])],\n",
    "    #    'clf__classifier__penalty': ['l2'],\n",
    "    #    'clf__classifier__C': [0.1, 1, 10],\n",
    "    \n",
    "\n",
    "    #}\n",
    "    #{\n",
    "    #    'clf': [BinaryRelevance(LogisticRegression())],\n",
    "    #    'clf__classifier__penalty': ['l1', 'l2'],\n",
    "    #    'clf__classifier__C': [0.1, 1, 10],\n",
    "    #    'clf__classifier__solver': ['liblinear', 'saga']\n",
    "    #}\n",
    "]\n",
    "\n",
    "make_pipeline(param_grid_binary_relevance, clf, pre_processing='word2vec')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem adaptation : Classifier Chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bcfe8c9e21b55aa01906cc0de6370d442d81bec12f7f116beb357c60c30ba9f9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
