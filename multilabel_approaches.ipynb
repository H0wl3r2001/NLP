{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing the data \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                data  score  rating  \\\n",
      "0  Contrairement aux idées reçues le traceur GPS ...      5       1   \n",
      "1               Application très pratique et fiable.      5       1   \n",
      "2                                   jadore ma montre      5       1   \n",
      "3  Super application, je l'utilise synchronisé av...      5       1   \n",
      "4                                            Super !      5       1   \n",
      "5  Application très pratique et très simple d'uti...      3       1   \n",
      "6  Suivis du sommeil cardio nombre de pas avec la...      5       1   \n",
      "7                                  Sympa et précis !      5       1   \n",
      "8                                     Très satisfait      5       1   \n",
      "9  bonjour, le calendrier ne se synchronise plus....      2       0   \n",
      "\n",
      "   bug_report  feature_request  user_experience  \n",
      "0           0                0                1  \n",
      "1           0                0                0  \n",
      "2           0                0                0  \n",
      "3           0                0                1  \n",
      "4           0                0                0  \n",
      "5           1                0                0  \n",
      "6           0                0                1  \n",
      "7           0                0                0  \n",
      "8           0                0                0  \n",
      "9           1                0                0  \n",
      "\n",
      " Number of rows: 6000\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "\n",
    "# Read the data from the file\n",
    "data_garmin_df = pd.read_csv('data/Garmin_Connect.csv')\n",
    "data_samsung_df = pd.read_csv('data/Samsung_Health.csv')\n",
    "data_huawei_df = pd.read_csv('data/Huawei_Health.csv')\n",
    "\n",
    "data = pd.concat([data_garmin_df, data_samsung_df, data_huawei_df], ignore_index=True)\n",
    "#data.to_csv('data/concatenated_data.csv', sep='\\t', encoding='utf-8')\n",
    "\n",
    "print(data.head(10))\n",
    "print(\"\\n Number of rows: \" + str(len(data)))\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization and removal of stopwords\n",
    "\n",
    "*Tokenization* is the process of splitting an input text into tokens (words or other relevant elements, such as punctuation, empty strings). We will use the result as a basis to predict a label.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Marta\n",
      "[nltk_data]     Mariz\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                data  score  rating  \\\n",
      "0  Contrairement aux idées reçues le traceur GPS ...      5       1   \n",
      "1               Application très pratique et fiable.      5       1   \n",
      "2                                   jadore ma montre      5       1   \n",
      "3  Super application, je l'utilise synchronisé av...      5       1   \n",
      "4                                            Super !      5       1   \n",
      "\n",
      "   bug_report  feature_request  user_experience  \\\n",
      "0           0                0                1   \n",
      "1           0                0                0   \n",
      "2           0                0                0   \n",
      "3           0                0                1   \n",
      "4           0                0                0   \n",
      "\n",
      "                                               token  \n",
      "0  contrair aux id e re ue le traceur gp est tr s...  \n",
      "1                      appliqu tr s pratiqu et fiabl  \n",
      "2                                     jador ma montr  \n",
      "3  sup appliqu je l utilis synchron avec ma fenix...  \n",
      "4                                                sup  \n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize #principal tokenization class from nltk API\n",
    "from nltk.stem import SnowballStemmer   #Stemming method\n",
    "import re                               #regex library\n",
    "nltk.download('punkt')\n",
    "\n",
    "## IMPLEMENT LEMMATIZATION\n",
    "\n",
    "corpus = []\n",
    "\n",
    "for index, row in data.iterrows():\n",
    "    review = re.sub('\\*', '', row[\"data\"]) # get data, substitute asterisks for empty string, put into review\n",
    "    review = re.sub('[^a-zA-Z]', ' ', review) # from review, remove all non-alphabetic characters\n",
    "    review = re.sub('[^\\w\\s]', '', review) # remove punctuation from review\n",
    "    review = ' '.join([SnowballStemmer('french').stem(w) for w in word_tokenize(review.lower(), language='french')]) # apply stemming\n",
    "    corpus.append(review)\n",
    "\n",
    "#print(corpus)\n",
    "\n",
    "data = data.assign(token=corpus)\n",
    "\n",
    "print(data.head())\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separation between train and test datasets\n",
    "\n",
    "Separate in adequate proportions to avoid the overfitting of the modules the data between features and targets. In this case there will be 2 different separations, one for the original multilabel problem and another for the mold into just a multiclass problem. To ensure a more even tag distribution, we must use the *stratify* hyper-parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4800,)\n",
      "(1200,)\n",
      "(4800, 4)\n",
      "(1200, 4)\n"
     ]
    }
   ],
   "source": [
    "from skmultilearn.model_selection import iterative_train_test_split\n",
    "from sklearn.model_selection import train_test_split\n",
    "x = data['token']  # try to include / exclude score and check if it yields better results\n",
    "y = data[['rating', 'bug_report', 'feature_request', 'user_experience']]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, stratify=y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Approaches to multilabel classification we will consider:\n",
    "\n",
    "#### Problem transformation\n",
    "1. Binary Relevance (consider each label as a separate single class classification  problem)\n",
    "2. Classifier Chains\n",
    "3. Label powerset\n",
    "\n",
    "#### Adapted Algorithms\n",
    "\n",
    "#### Ensemble methods"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TF-IDF Vectorizer setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Marta\n",
      "[nltk_data]     Mariz\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "final_stopwords_list = stopwords.words('english') + stopwords.words('french')\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.5,\n",
    "                                   max_features=200000,\n",
    "                                   min_df=0.2,\n",
    "                                   stop_words=final_stopwords_list,\n",
    "                                   use_idf=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text preprocessing, tokenizing and filtering of stopwords are all included in CountVectorizer, which builds a dictionary of features and transforms documents to feature vectors; "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn import metrics\n",
    "import pandas as pd\n",
    "from sklearn.svm import LinearSVC\n",
    "# Define the hyperparameters to tune for the TfidfTransformer, which will be the same for every method approach as this preprocessor is always used\n",
    "tfidf_params = {\n",
    "    'use_idf': [True, False],\n",
    "    'smooth_idf': [True, False],\n",
    "    'sublinear_tf': [True, False],\n",
    "    'norm': ['l1', 'l2']\n",
    "}\n",
    "\n",
    "#Define the hyperparameters to tune for the CountVectorizer, which will be the same for every method approach as this preprocessor is always used\n",
    "count_vectorizer_params = {\n",
    "    'max_df': [0.5, 0.75, 1.0],\n",
    "    'min_df': [0.2, 0.25, 0.3],\n",
    "    'max_features': [None, 1000, 2000, 3000],\n",
    "    'ngram_range': [(1, 1), (1, 2), (1, 3)],\n",
    "    'stop_words': [None, 'english', final_stopwords_list],\n",
    "}\n",
    "\n",
    "def make_pipeline(clf_param_grid, clf):\n",
    "    pipeline = Pipeline([\n",
    "        ('vect', CountVectorizer()),\n",
    "        ('tfidf', TfidfTransformer()),\n",
    "        ('clf', clf)\n",
    "    ])\n",
    "\n",
    "    # Define the GridSearchCV object to tune the hyperparameters\n",
    "    grid_params = [\n",
    "        {\n",
    "        'vect__max_df': count_vectorizer_params['max_df'],\n",
    "        'vect__ngram_range': count_vectorizer_params['ngram_range'],\n",
    "        'vect__min_df': count_vectorizer_params['min_df'],\n",
    "        'vect__stop_words': count_vectorizer_params['stop_words'],\n",
    "        'tfidf__use_idf': tfidf_params['use_idf'],\n",
    "        'tfidf__smooth_idf': tfidf_params['smooth_idf'],\n",
    "        'tfidf__sublinear_tf': tfidf_params['sublinear_tf'],\n",
    "        'tfidf__norm': tfidf_params['norm'],\n",
    "        **clf_param\n",
    "        }\n",
    "        for clf_param in clf_param_grid\n",
    "    ]\n",
    "\n",
    "    # Define the GridSearchCV object\n",
    "    grid_search = GridSearchCV(pipeline, param_grid=grid_params, cv=5, n_jobs=-1)\n",
    "\n",
    "\n",
    "    # Fit the GridSearchCV object to the training data\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "    # Get the best hyperparameters and best model\n",
    "    best_params = grid_search.best_params_\n",
    "    best_model = grid_search.best_estimator_\n",
    "    print(\"Best parameters: \", best_params)\n",
    "    print(\"Best model: \", best_model)\n",
    "\n",
    "    # Evaluate the best model on the test data\n",
    "    y_pred = best_model.predict(X_test)\n",
    "    #Apply classification report\n",
    "    print(metrics.classification_report(y_test, y_pred, digits=3))\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem transformation: Binary Relevance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using binary relevance\n",
    "from skmultilearn.problem_transform import BinaryRelevance\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clf = BinaryRelevance(GaussianNB())\n",
    "\n",
    "param_grid_binary_relevance = [\n",
    "    {\n",
    "        'clf': [clf],\n",
    "        'clf__classifier__var_smoothing': [1e-9, 1e-8, 1e-7, 1e-6, 1e-5]\n",
    "    },\n",
    "    {\n",
    "        'clf': [BinaryRelevance(RandomForestClassifier())],\n",
    "        'clf__classifier__n_estimators': [50, 100],\n",
    "        'clf__classifier__max_depth': [10, 20],\n",
    "    },\n",
    "    {\n",
    "        'clf': [BinaryRelevance(LogisticRegression())],\n",
    "        'clf__classifier__penalty': ['l1', 'l2'],\n",
    "        'clf__classifier__C': [0.1, 1, 10],\n",
    "        'clf__classifier__solver': ['liblinear', 'saga']\n",
    "    }\n",
    "]\n",
    "\n",
    "make_pipeline(param_grid_binary_relevance, clf)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bcfe8c9e21b55aa01906cc0de6370d442d81bec12f7f116beb357c60c30ba9f9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
